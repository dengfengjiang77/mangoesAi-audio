{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "In this code, we fine-tune the MIT speech commands V2, add \"Hey ZZX\" to the model, after that we can use this new model to recognize our new wakeup words\n",
    "Procedure:\n",
    "- Generate the new voice dataset ( at least 30 samples in different voice), you can use text to speech tools to generate\n",
    "  - https://www.narakeet.com/languages/chinese-text-to-speech\n",
    "  - https://micmonster.com/text-to-speech/chinese-mandarin-simplified/\n",
    "- Covert the voice to required type (wav file, 16kHz voice) - use code\n",
    "- Split it to training dataset(80%) and test dataset(20%), if you have more data , leave some validation dataset  - use code\n",
    "- generate two csv file(train and test), with their links\n",
    "- Start to Train , save the model to local PC\n",
    "- Realtime test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: Skip generate voice, covert type, split to csv\n",
    "Start from training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "from transformers import AutoProcessor, AutoModelForAudioClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# loading data\n",
    "data_files = {\n",
    "    \"train\": \"/Users/7one/Documents/Work/mangoesai/livekit_paddle/heyzzx2/train.csv\",\n",
    "    \"test\": \"/Users/7one/Documents/Work/mangoesai/livekit_paddle/heyzzx2/test.csv\",\n",
    "}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "# convert label to string\n",
    "def convert_label_to_str(batch):\n",
    "    batch[\"label\"] = str(batch[\"label\"])\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.map(convert_label_to_str, num_proc=1) \n",
    "\n",
    "# loading classification model and processor\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    \"MIT/ast-finetuned-speech-commands-v2\",\n",
    "    num_labels=2,  # modify to your classification number\n",
    "    ignore_mismatched_sizes=True  # ignore size mismatch error\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\"MIT/ast-finetuned-speech-commands-v2\")\n",
    "\n",
    "# data preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    try:\n",
    "        audio_array = batch[\"path\"][\"array\"]\n",
    "        inputs = processor(audio_array, sampling_rate=16000, return_tensors=\"pt\")\n",
    "        batch[\"input_values\"] = inputs.input_values[0].numpy()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {batch}: {e}\")\n",
    "        batch[\"input_values\"] = None\n",
    "    return batch\n",
    "\n",
    "dataset = dataset.cast_column(\"path\", Audio(sampling_rate=16000))\n",
    "dataset = dataset.map(preprocess_audio, remove_columns=[\"path\", \"text\"], num_proc=1)\n",
    "\n",
    "# custom data collator\n",
    "def data_collator(features):\n",
    "    input_values = torch.tensor([f[\"input_values\"] for f in features], dtype=torch.float32)\n",
    "    labels = torch.tensor([int(f[\"label\"]) for f in features], dtype=torch.long)  # 确保是整数类型\n",
    "    return {\"input_values\": input_values, \"labels\": labels}\n",
    "\n",
    "# set training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./audio-classification-hey-zzx\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    save_steps=500,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# define Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=processor,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()\n",
    "\n",
    "model.config.id2label = {0: \"Not Hey ZZX\", 1: \"Hey ZZX\"}\n",
    "model.config.label2id = {\"Not Hey ZZX\": 0, \"Hey ZZX\": 1}\n",
    "\n",
    "\n",
    "model.save_pretrained(\"/Users/7one/Documents/Work/mangoesai/livekit_paddle/heyzzx2\")\n",
    "processor.save_pretrained(\"/Users/7one/Documents/Work/mangoesai/livekit_paddle/heyzzx2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please be aware of several things:\n",
    "1- training data should be exactly correct, please print debug info for the data in details\n",
    "2- save the model properly ( you can change the path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test1 - specific wav file test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# load classifier\n",
    "classifier = pipeline(\"audio-classification\", model=\"/Users/7one/Documents/Work/mangoesai/livekit_paddle/heyzzx2\")\n",
    "\n",
    "# test audio path\n",
    "audio_path = \"/Users/7one/Documents/Work/mangoesai/livekit_paddle/heyzzx/voicedataset/converted_wav/test9.wav\"\n",
    "\n",
    "# use classifier to predict\n",
    "prediction = classifier(audio_path)\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test2 - realtime test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for wake word 'Hey ZZX'...\n",
      "Detected label: Hey ZZX\n",
      "Wake word 'Hey ZZX' detected!\n",
      "Welcome, ZZX, what can I do for you?\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForAudioClassification\n",
    "\n",
    "# loading\n",
    "model_path = \"/Users/7one/Documents/Work/mangoesai/livekit_paddle/heyzzx2\"\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model = AutoModelForAudioClassification.from_pretrained(model_path)\n",
    "model.eval()\n",
    "\n",
    "# audio parameters\n",
    "sample_rate = 16000  # model needed sampling rate\n",
    "chunk_size = 16000   # capture 1 second audio (16000 samples)\n",
    "silence_threshold = 0.01  # silence detection threshold\n",
    "\n",
    "# initialize PyAudio\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=pyaudio.paInt16,\n",
    "                channels=1,\n",
    "                rate=sample_rate,\n",
    "                input=True,\n",
    "                frames_per_buffer=chunk_size)\n",
    "\n",
    "def classify_audio(audio_array):\n",
    "    # pre-processing voice\n",
    "    inputs = processor(audio_array, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    return model.config.id2label[predicted_label]\n",
    "\n",
    "def is_silent(audio_array):\n",
    "    \"\"\"\n",
    "    determine if the audio is silent\n",
    "    Args:\n",
    "        audio_array (np.ndarray): audio data\n",
    "    Returns:\n",
    "        bool: whether the audio is silent\n",
    "    \"\"\"\n",
    "    return np.max(np.abs(audio_array)) < silence_threshold\n",
    "\n",
    "print(\"Listening for wake word 'Hey ZZX'...\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # capture audio data\n",
    "        raw_audio = stream.read(chunk_size, exception_on_overflow=False)\n",
    "        audio_array = np.frombuffer(raw_audio, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "        # skip silent\n",
    "        if is_silent(audio_array):\n",
    "            print(\"Silent audio detected. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # use classification model to detect audio\n",
    "        predicted_label = classify_audio(audio_array)\n",
    "        print(f\"Detected label: {predicted_label}\")\n",
    "\n",
    "        # check if it is wake word\n",
    "        if predicted_label.lower() == \"hey zzx\":\n",
    "            print(\"Wake word 'Hey ZZX' detected!\")\n",
    "            print(\"Welcome, ZZX, what can I do for you?\")\n",
    "            break\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Stopping...\")\n",
    "\n",
    "finally:\n",
    "    # close audio stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
