{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/var/folders/rp/s3c7_7cn6sg2s70tp08w3xwm0000gn/T/ipykernel_54608/4217213878.py:2: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# 加载预训练的语音嵌入模型\n",
    "speaker_model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "# 存储注册用户的嵌入向量\n",
    "registered_speakers = {}\n",
    "\n",
    "def register_speaker(audio_path, speaker_name):\n",
    "    \"\"\"注册用户声音\"\"\"\n",
    "    # 加载音频\n",
    "    audio_signal = torch.tensor([load_audio(audio_path)])\n",
    "\n",
    "    embeddings = speaker_model.encode_batch(audio_signal)\n",
    "    embedding_vector = embeddings.squeeze().detach().numpy()\n",
    "    registered_speakers[speaker_name] = embedding_vector\n",
    "    print(f\"Speaker '{speaker_name}' registered successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_speaker(audio_path, threshold=0.4):\n",
    "    \"\"\"验证用户声音是否匹配\"\"\"\n",
    "    if not registered_speakers:\n",
    "        print(\"No registered speakers found. Please register speakers first.\")\n",
    "        return None\n",
    "\n",
    "    # 提取待验证音频的嵌入\n",
    "    audio_signal = torch.tensor([load_audio(audio_path)])\n",
    "    test_embeddings = speaker_model.encode_batch(audio_signal).squeeze().detach().numpy()\n",
    "\n",
    "    # 逐个用户计算相似度\n",
    "    best_match = None\n",
    "    best_score = float(\"-inf\")\n",
    "    for speaker_name, embedding_vector in registered_speakers.items():\n",
    "        similarity = 1 - cosine(test_embeddings, embedding_vector)\n",
    "        print(f\"Similarity score with '{speaker_name}': {similarity}\")\n",
    "\n",
    "        # 找到最相似的用户\n",
    "        if similarity > best_score:\n",
    "            best_score = similarity\n",
    "            best_match = speaker_name\n",
    "\n",
    "    # 判定结果\n",
    "    if best_score >= threshold:\n",
    "        print(f\"Voice is from '{best_match}', welcome back!\")\n",
    "        return best_match\n",
    "    else:\n",
    "        print(\"Voice not recognized.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def load_audio(audio_path):\n",
    "    \"\"\"加载音频文件并转换为特定采样率\"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resample_transform(waveform)\n",
    "    return waveform.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "def save_to_temp_wav(audio_data, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    将音频字节流保存为临时 WAV 文件\n",
    "    Args:\n",
    "        audio_data (bytes): 音频数据字节流\n",
    "        sample_rate (int): 音频采样率，默认为 16kHz\n",
    "    Returns:\n",
    "        str: 保存的临时 WAV 文件路径\n",
    "    \"\"\"\n",
    "    # 创建一个临时文件\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "    temp_file.close()\n",
    "\n",
    "    # 将字节流写入 WAV 文件\n",
    "    with sf.SoundFile(temp_file.name, mode=\"w\", samplerate=sample_rate, channels=1, subtype=\"PCM_16\") as f:\n",
    "        # 将字节流转换为 NumPy 数组\n",
    "        audio_array = np.frombuffer(audio_data, dtype=np.int16)\n",
    "        f.write(audio_array)\n",
    "    \n",
    "    return temp_file.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_cache = []  # 全局音频缓存\n",
    "\n",
    "def verify_realtime_speaker(audio_chunk, speaker_name, threshold=0.1, min_duration=3.0, adaptive=True):\n",
    "    \"\"\"\n",
    "    实时音频的说话人验证，支持累积音频和动态阈值调整\n",
    "    \"\"\"\n",
    "    global audio_cache\n",
    "\n",
    "    if speaker_name not in registered_speakers:\n",
    "        print(f\"Error: Speaker '{speaker_name}' not registered.\")\n",
    "        return False\n",
    "\n",
    "    audio_cache.append(audio_chunk)\n",
    "    total_audio = b\"\".join(audio_cache)\n",
    "    duration = len(total_audio) / (16000 * 2)  # 累积音频时长\n",
    "\n",
    "    if duration >= min_duration:\n",
    "        print(\"Performing speaker verification with accumulated audio...\")\n",
    "        try:\n",
    "            audio_array = np.frombuffer(total_audio, dtype=np.int16)\n",
    "            embeddings = speaker_model.encode_batch(torch.tensor([audio_array], dtype=torch.float32))\n",
    "            embeddings = embeddings.squeeze().detach().numpy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during embedding extraction: {e}\")\n",
    "            return False\n",
    "\n",
    "        similarity = 1 - cosine(embeddings, registered_speakers[speaker_name])\n",
    "        print(f\"Similarity score with '{speaker_name}': {similarity}\")\n",
    "\n",
    "        # 动态调整阈值\n",
    "        adjusted_threshold = threshold\n",
    "        if adaptive:\n",
    "            adjusted_threshold = max(threshold, similarity - 0.1)\n",
    "\n",
    "        audio_cache = []  # 清空缓存\n",
    "        return similarity > adjusted_threshold\n",
    "\n",
    "    return False  # 音频累积不足\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "def save_realtime_audio(audio_cache, sample_rate=16000, output_path=\"/Users/7one/Documents/Work/mangoesai/livekit_paddle/realtime_audio_debug.wav\"):\n",
    "    \"\"\"\n",
    "    将缓存的实时音频保存为 WAV 文件\n",
    "    Args:\n",
    "        audio_cache (list): 实时录音缓存，每个元素是 bytes 类型\n",
    "        sample_rate (int): 采样率，默认为 16kHz\n",
    "        output_path (str): 保存的音频文件路径\n",
    "    \"\"\"\n",
    "    # 合并所有音频块\n",
    "    full_audio_data = b\"\".join(audio_cache)\n",
    "\n",
    "    # 将字节流转换为 NumPy 数组\n",
    "    audio_array = np.frombuffer(full_audio_data, dtype=np.int16)\n",
    "\n",
    "    # 保存为 WAV 文件\n",
    "    sf.write(output_path, audio_array, samplerate=sample_rate, subtype=\"PCM_16\")\n",
    "    print(f\"Realtime audio saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (3529796771.py, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 77\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Voice is from '{best_match}', welcome back, Say \"Hi ZZX\"!\")\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines.audio_utils import ffmpeg_microphone_live\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# 保存实时音频的函数\n",
    "def save_realtime_audio(audio_cache, sample_rate=16000, output_path=\"realtime_audio_debug.wav\"):\n",
    "    \"\"\"\n",
    "    将缓存的实时音频保存为 WAV 文件\n",
    "    Args:\n",
    "        audio_cache (list): 实时录音缓存，每个元素是 bytes 类型\n",
    "        sample_rate (int): 采样率，默认为 16kHz\n",
    "        output_path (str): 保存的音频文件路径\n",
    "    \"\"\"\n",
    "    # 合并所有音频块\n",
    "    full_audio_data = b\"\".join(audio_cache)\n",
    "\n",
    "    # 将字节流转换为 NumPy 数组\n",
    "    audio_array = np.frombuffer(full_audio_data, dtype=np.int16)\n",
    "\n",
    "    # 保存为 WAV 文件\n",
    "    sf.write(output_path, audio_array, samplerate=sample_rate, subtype=\"PCM_16\")\n",
    "    print(f\"Realtime audio saved to: {output_path}\")\n",
    "\n",
    "\n",
    "# 实时录音、验证和转录的主函数\n",
    "def transcribe_with_speaker_verification(chunk_length_s=10.0, stream_chunk_s=1.0, max_duration=30, threshold=0.4):\n",
    "    mic = ffmpeg_microphone_live(\n",
    "        sampling_rate=16000,\n",
    "        chunk_length_s=chunk_length_s,\n",
    "        stream_chunk_s=stream_chunk_s,\n",
    "    )\n",
    "\n",
    "    print(f\"Listening for speaker ...\")\n",
    "    audio_cache = []  # 用于缓存音频块\n",
    "    full_audio = b\"\"  # 用于累积完整音频\n",
    "    identified_user = None  # 最终识别的用户\n",
    "    start_time = time.time()  # 记录开始时间\n",
    "\n",
    "    try:\n",
    "        for i, audio_chunk in enumerate(mic):\n",
    "            elapsed_time = time.time() - start_time  # 计算已运行时间\n",
    "            if elapsed_time >= max_duration:\n",
    "                print(\"Reached max duration, stopping recording.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Processing chunk {i + 1} (elapsed time: {elapsed_time:.2f}s)...\")\n",
    "            raw_audio = audio_chunk[\"raw\"]  # 获取音频字节流\n",
    "\n",
    "            # 检查并转换音频块为字节流\n",
    "            if isinstance(raw_audio, np.ndarray):\n",
    "                if raw_audio.dtype == np.float32:  # 如果是浮点数，转换为 int16\n",
    "                    raw_audio = (raw_audio * 32768).astype(np.int16)\n",
    "                raw_audio = raw_audio.tobytes()\n",
    "            \n",
    "            # 缓存音频块\n",
    "            audio_cache.append(raw_audio)\n",
    "            full_audio += raw_audio  # 累积完整音频\n",
    "\n",
    "            # 验证用户声音\n",
    "            audio_array = np.frombuffer(raw_audio, dtype=np.int16)\n",
    "            similarity_scores = {}\n",
    "            for speaker_name, embedding_vector in registered_speakers.items():\n",
    "                test_embeddings = speaker_model.encode_batch(torch.tensor([audio_array]))\n",
    "                test_embeddings = test_embeddings.squeeze().detach().numpy()\n",
    "                similarity = 1 - cosine(test_embeddings, embedding_vector)\n",
    "                similarity_scores[speaker_name] = similarity\n",
    "            \n",
    "            # 找出最相似的用户\n",
    "            best_match = max(similarity_scores, key=similarity_scores.get)\n",
    "            best_score = similarity_scores[best_match]\n",
    "            print(f\"Similarity score with '{best_match}': {best_score}\")\n",
    "\n",
    "            if best_score >= threshold:\n",
    "                print(f\"Voice is from '{best_match}', welcome back, Say 'Hi ZZX'!\")\n",
    "                identified_user = best_match\n",
    "                break\n",
    "            else:\n",
    "                print(\"Voice not recognized. Continuing to listen...\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Recording interrupted by user.\")\n",
    "\n",
    "    finally:\n",
    "        # 保存缓存的音频（无论验证是否成功）\n",
    "        if audio_cache:\n",
    "            output_path = \"/Users/7one/Documents/Work/mangoesai/livekit_paddle/realtime_audio_debug.wav\"\n",
    "            save_realtime_audio(audio_cache, sample_rate=16000, output_path=output_path)\n",
    "\n",
    "        if identified_user:\n",
    "            return identified_user\n",
    "        else:\n",
    "            print(\"No speaker identified.\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_speaker(\"/Users/7one/Documents/Work/mangoesai/livekit_paddle/99.wav\", \"Dengfeng\")\n",
    "register_speaker(\"/Users/7one/Documents/Work/mangoesai/livekit_paddle/amit.wav\", \"Amit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "identified_user = transcribe_with_speaker_verification(\n",
    "    chunk_length_s=10.0,\n",
    "    stream_chunk_s=1.0,\n",
    "    max_duration=30,\n",
    "    threshold=0.4\n",
    ")\n",
    "if identified_user:\n",
    "    print(f\"Hello, {identified_user}!\")\n",
    "else:\n",
    "    print(\"No speaker recognized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"audio-classification\", model=\"MIT/ast-finetuned-speech-commands-v2\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.model.config.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.audio_utils import ffmpeg_microphone_live\n",
    "\n",
    "\n",
    "def launch_fn(\n",
    "    wake_word=\"marvin\",\n",
    "    prob_threshold=0.5,\n",
    "    chunk_length_s=2.0,\n",
    "    stream_chunk_s=0.25,\n",
    "    debug=False,\n",
    "):\n",
    "    if wake_word not in classifier.model.config.label2id.keys():\n",
    "        raise ValueError(\n",
    "            f\"Wake word {wake_word} not in set of valid class labels, pick a wake word in the set {classifier.model.config.label2id.keys()}.\"\n",
    "        )\n",
    "\n",
    "    sampling_rate = classifier.feature_extractor.sampling_rate\n",
    "\n",
    "    mic = ffmpeg_microphone_live(\n",
    "        sampling_rate=sampling_rate,\n",
    "        chunk_length_s=chunk_length_s,\n",
    "        stream_chunk_s=stream_chunk_s,\n",
    "    )\n",
    "\n",
    "    print(\"Listening for wake word...\")\n",
    "    for prediction in classifier(mic):\n",
    "        prediction = prediction[0]\n",
    "        if debug:\n",
    "            print(prediction)\n",
    "        if prediction[\"label\"] == wake_word:\n",
    "            if prediction[\"score\"] > prob_threshold:\n",
    "                return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_fn(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = pipeline(\n",
    "    \"automatic-speech-recognition\", model=\"openai/whisper-small.en\", device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def transcribe(chunk_length_s=10.0, stream_chunk_s=2.0):\n",
    "    sampling_rate = transcriber.feature_extractor.sampling_rate\n",
    "\n",
    "    mic = ffmpeg_microphone_live(\n",
    "        sampling_rate=sampling_rate,\n",
    "        chunk_length_s=chunk_length_s,\n",
    "        stream_chunk_s=stream_chunk_s,\n",
    "    )\n",
    "\n",
    "    print(\"Start speaking...\")\n",
    "    for item in transcriber(mic, generate_kwargs={\"max_new_tokens\": 256}):\n",
    "        sys.stdout.write(\"\\033[K\")\n",
    "        print(item[\"text\"], end=\"\\r\")\n",
    "        if not item[\"partial\"][0]:\n",
    "            break\n",
    "\n",
    "    return item[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfFolder\n",
    "import requests\n",
    "\n",
    "\n",
    "def query(text, model_id=\"tiiuae/falcon-7b-instruct\"):\n",
    "    api_url = f\"https://api-inference.huggingface.co/models/{model_id}\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HfFolder().get_token()}\"}\n",
    "    payload = {\"inputs\": text}\n",
    "\n",
    "    print(f\"Querying...: {text}\")\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    return response.json()[0][\"generated_text\"][len(text) + 1 :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query(\"What does Hugging Face do?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan\n",
    "\n",
    "processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\").to(device)\n",
    "vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesise(text):\n",
    "    inputs = processor(text=text, return_tensors=\"pt\")\n",
    "    speech = model.generate_speech(\n",
    "        inputs[\"input_ids\"].to(device), speaker_embeddings.to(device), vocoder=vocoder\n",
    "    )\n",
    "    return speech.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "audio = synthesise(\n",
    "    \"Hugging Face is a company that provides natural language processing and machine learning tools for developers.\"\n",
    ")\n",
    "\n",
    "Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "# Step 1: 说话人验证\n",
    "transcribe_with_speaker_verification(\n",
    "    chunk_length_s=10.0,\n",
    "    stream_chunk_s=1.0,\n",
    "    max_duration=10,\n",
    "    speaker_name=\"dengfeng\"\n",
    ")\n",
    "\n",
    "if not transcription:\n",
    "    print(\"Speaker verification failed or no audio recorded. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Step 2: 唤醒词检测\n",
    "if launch_fn():\n",
    "    print(\"Wake word detected. Proceeding to transcription...\")\n",
    "else:\n",
    "    print(\"No wake word detected. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Step 3: 实时语音转录\n",
    "transcription = transcribe()\n",
    "if not transcription:\n",
    "    print(\"No transcription available. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Step 4: 调用 LLM 生成回复\n",
    "response = query(transcription)\n",
    "if not response:\n",
    "    print(\"No response from LLM. Exiting...\")\n",
    "    exit()\n",
    "\n",
    "# Step 5: 将 LLM 回复转为语音\n",
    "audio = synthesise(response)\n",
    "\n",
    "# Step 6: 播放合成语音\n",
    "Audio(audio, rate=16000, autoplay=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
