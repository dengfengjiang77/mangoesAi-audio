{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is showing the how we processing the specific video/voice input and output with the specific names of speakers, the steps are as following:\n",
    "- loading embedding model and store the known voice dataset into embedding warehouse\n",
    "- Perform diarization the target video, using pyannote.audio to split and get the slices of voice\n",
    "- calculate the speaker's embeddings and compare it with the known embedding\n",
    "- generate transcribed text and label it with identified speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1 - Load the embedding model & store known voice dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines import PretrainedSpeakerEmbedding\n",
    "import torchaudio\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained speaker embedding model\n",
    "embedding_model = PretrainedSpeakerEmbedding(\n",
    "    \"pyannote/embedding\", \n",
    "    device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    ")\n",
    "\n",
    "# Function to compute embeddings\n",
    "def get_speaker_embedding(audio_path):\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    return embedding_model({'waveform': waveform, 'sample_rate': sample_rate})\n",
    "\n",
    "# Store known speaker embeddings\n",
    "known_speakers = {\n",
    "    \"Ryan\": get_speaker_embedding(\"Ryan.wav\"),\n",
    "    \"Jackie\": get_speaker_embedding(\"Jackie.wav\"),\n",
    "    \"Rebecca\": get_speaker_embedding(\"Rebecca.wav\"),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step2 - Diarization(speaker segmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyannote.audio.pipelines import SpeakerDiarization\n",
    "from pyannote.core import Segment\n",
    "import wave\n",
    "\n",
    "# Load diarization model\n",
    "diarization_pipeline = SpeakerDiarization.from_pretrained(\"pyannote/speaker-diarization\")\n",
    "\n",
    "# Process the target video/audio file\n",
    "audio_file = \"target_video_audio.wav\"\n",
    "diarization_result = diarization_pipeline(audio_file)\n",
    "\n",
    "# Extract speaker-wise audio segments\n",
    "speaker_segments = []\n",
    "for turn, _, speaker in diarization_result.itertracks(yield_label=True):\n",
    "    speaker_segments.append({\n",
    "        \"speaker\": speaker,\n",
    "        \"start\": turn.start,\n",
    "        \"end\": turn.end\n",
    "    })\n",
    "\n",
    "# Save individual speaker segments as separate audio files\n",
    "import torchaudio\n",
    "\n",
    "def extract_audio_segment(input_audio, start_time, end_time, output_file):\n",
    "    waveform, sample_rate = torchaudio.load(input_audio)\n",
    "    start_sample = int(start_time * sample_rate)\n",
    "    end_sample = int(end_time * sample_rate)\n",
    "    torchaudio.save(output_file, waveform[:, start_sample:end_sample], sample_rate)\n",
    "\n",
    "# Save each diarized segment for speaker identification\n",
    "for idx, segment in enumerate(speaker_segments):\n",
    "    output_path = f\"speaker_segment_{idx}.wav\"\n",
    "    extract_audio_segment(audio_file, segment[\"start\"], segment[\"end\"], output_path)\n",
    "    segment[\"audio_path\"] = output_path  # Store the extracted file path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step3 - Compare Speaker Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Function to match an unknown speaker with the closest known speaker\n",
    "def match_speaker(unknown_embedding):\n",
    "    best_match = None\n",
    "    best_score = float(\"inf\")  # Cosine distance: lower = better match\n",
    "\n",
    "    for name, known_embedding in known_speakers.items():\n",
    "        score = cosine(unknown_embedding, known_embedding)\n",
    "        if score < best_score:\n",
    "            best_match = name\n",
    "            best_score = score\n",
    "\n",
    "    # Set threshold for reliable speaker identification (e.g., 90% similarity = cosine < 0.1)\n",
    "    return best_match if best_score < 0.1 else \"UNKNOWN\"\n",
    "\n",
    "# Identify speakers for each diarized segment\n",
    "speaker_mapping = {}\n",
    "for segment in speaker_segments:\n",
    "    audio_path = segment[\"audio_path\"]\n",
    "    speaker_id = segment[\"speaker\"]\n",
    "\n",
    "    embedding = get_speaker_embedding(audio_path)\n",
    "    matched_name = match_speaker(embedding)\n",
    "\n",
    "    speaker_mapping[speaker_id] = matched_name\n",
    "\n",
    "print(\"Speaker Mapping:\", speaker_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step4 - Generate the transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "# Load Whisper model\n",
    "whisper_model = whisper.load_model(\"medium\")\n",
    "\n",
    "# Transcribe audio\n",
    "transcription = whisper_model.transcribe(audio_file)\n",
    "\n",
    "# Convert transcript into speaker-labeled format\n",
    "final_transcript = []\n",
    "for segment in transcription[\"segments\"]:\n",
    "    start = segment[\"start\"]\n",
    "    end = segment[\"end\"]\n",
    "    text = segment[\"text\"]\n",
    "\n",
    "    # Find the speaker for this segment based on diarization timestamps\n",
    "    assigned_speaker = \"UNKNOWN\"\n",
    "    for diarization_segment in speaker_segments:\n",
    "        if diarization_segment[\"start\"] <= start <= diarization_segment[\"end\"]:\n",
    "            assigned_speaker = speaker_mapping.get(diarization_segment[\"speaker\"], \"UNKNOWN\")\n",
    "            break\n",
    "\n",
    "    final_transcript.append(f\"{assigned_speaker}: {text}\")\n",
    "\n",
    "# Print the final transcribed conversation\n",
    "for line in final_transcript:\n",
    "    print(line)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
