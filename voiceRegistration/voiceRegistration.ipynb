{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voice Registration\n",
    "- model used: Speechbrain \n",
    "- procedure : \n",
    "  - register the user's voice (3~6s voice is needed)\n",
    "  - verify the voice\n",
    "    - capture the real-time voice\n",
    "    - load the voice\n",
    "    - verify with scores\n",
    "    - return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:speechbrain.utils.quirks:Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]\n",
      "INFO:speechbrain.utils.quirks:Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []\n",
      "/var/folders/rp/s3c7_7cn6sg2s70tp08w3xwm0000gn/T/ipykernel_93328/4217213878.py:2: UserWarning: Module 'speechbrain.pretrained' was deprecated, redirecting to 'speechbrain.inference'. Please update your script. This is a change from SpeechBrain 1.0. See: https://github.com/speechbrain/speechbrain/releases/tag/v1.0.0\n",
      "  from speechbrain.pretrained import EncoderClassifier\n",
      "INFO:speechbrain.utils.fetching:Fetch hyperparams.yaml: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch custom.py: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speechbrain/utils/autocast.py:68: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n",
      "INFO:speechbrain.utils.fetching:Fetch embedding_model.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch mean_var_norm_emb.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch classifier.ckpt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.fetching:Fetch label_encoder.txt: Fetching from HuggingFace Hub 'speechbrain/spkrec-ecapa-voxceleb' if not cached\n",
      "INFO:speechbrain.utils.parameter_transfer:Loading pretrained files for: embedding_model, mean_var_norm_emb, classifier, label_encoder\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speechbrain/utils/checkpoints.py:200: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(path, map_location=device)\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/speechbrain/processing/features.py:1311: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# loading\n",
    "speaker_model = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
    "\n",
    "# store registered speaker's embedding vector\n",
    "registered_speakers = {}\n",
    "\n",
    "def register_speaker(audio_path, speaker_name):\n",
    "    \"\"\"register speaker's voice\"\"\"\n",
    "    # load audio\n",
    "    audio_signal = torch.tensor([load_audio(audio_path)])\n",
    "\n",
    "    embeddings = speaker_model.encode_batch(audio_signal)\n",
    "    embedding_vector = embeddings.squeeze().detach().numpy()\n",
    "    registered_speakers[speaker_name] = embedding_vector\n",
    "    print(f\"Speaker '{speaker_name}' registered successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def load_audio(audio_path):\n",
    "    \"\"\"load audio file and convert to specific sample rate\"\"\"\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resample_transform(waveform)\n",
    "    return waveform.squeeze().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "def save_to_temp_wav(audio_data, sample_rate=16000):\n",
    "    \"\"\"\n",
    "        save audio bytes stream to temporary WAV file\n",
    "        Args:\n",
    "            audio_data (bytes): audio data bytes stream\n",
    "            sample_rate (int): audio sample rate, default 16kHz\n",
    "        Returns:\n",
    "            str: path of the temporary WAV file\n",
    "    \"\"\"\n",
    "    # create a temporary file\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\")\n",
    "    temp_file.close()\n",
    "\n",
    "    # write audio bytes stream to WAV file\n",
    "    with sf.SoundFile(temp_file.name, mode=\"w\", samplerate=sample_rate, channels=1, subtype=\"PCM_16\") as f:\n",
    "        # convert bytes stream to NumPy array\n",
    "        audio_array = np.frombuffer(audio_data, dtype=np.int16)\n",
    "        f.write(audio_array)\n",
    "    \n",
    "    return temp_file.name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_cache = []  # overall audio cache\n",
    "\n",
    "def verify_realtime_speaker(audio_chunk, speaker_name, threshold=0.1, min_duration=3.0, adaptive=True):\n",
    "    \"\"\"\n",
    "    real-time speaker verification, support cumulative audio and dynamic threshold adjustment\n",
    "    \"\"\"\n",
    "    global audio_cache\n",
    "\n",
    "    if speaker_name not in registered_speakers:\n",
    "        print(f\"Error: Speaker '{speaker_name}' not registered.\")\n",
    "        return False\n",
    "\n",
    "    audio_cache.append(audio_chunk)\n",
    "    total_audio = b\"\".join(audio_cache)\n",
    "    duration = len(total_audio) / (16000 * 2)  # cumulative audio duration\n",
    "\n",
    "    if duration >= min_duration:\n",
    "        print(\"Performing speaker verification with accumulated audio...\")\n",
    "        try:\n",
    "            audio_array = np.frombuffer(total_audio, dtype=np.int16)\n",
    "            embeddings = speaker_model.encode_batch(torch.tensor([audio_array], dtype=torch.float32))\n",
    "            embeddings = embeddings.squeeze().detach().numpy()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during embedding extraction: {e}\")\n",
    "            return False\n",
    "\n",
    "        similarity = 1 - cosine(embeddings, registered_speakers[speaker_name])\n",
    "        print(f\"Similarity score with '{speaker_name}': {similarity}\")\n",
    "\n",
    "        # dynamic adjust threshold\n",
    "        adjusted_threshold = threshold\n",
    "        if adaptive:\n",
    "            adjusted_threshold = max(threshold, similarity - 0.1)\n",
    "\n",
    "        audio_cache = []  # clear cache\n",
    "        return similarity > adjusted_threshold\n",
    "\n",
    "    return False  # audio accumulation is insufficient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import numpy as np\n",
    "\n",
    "def save_realtime_audio(audio_cache, sample_rate=16000, output_path=\"/Users/7one/Documents/Work/mangoesai/livekit_paddle/realtime_audio_debug.wav\"):\n",
    "    \"\"\"\n",
    "    save real-time audio to WAV file\n",
    "    Args:\n",
    "        audio_cache (list): real-time recording cache, each element is bytes type\n",
    "        sample_rate (int): audio sample rate, default 16kHz\n",
    "        output_path (str): path of the audio file\n",
    "    \"\"\"\n",
    "    # merge all audio blocks\n",
    "    full_audio_data = b\"\".join(audio_cache)\n",
    "\n",
    "    # convert bytes stream to NumPy array\n",
    "    audio_array = np.frombuffer(full_audio_data, dtype=np.int16)\n",
    "\n",
    "    # save as WAV file\n",
    "    sf.write(output_path, audio_array, samplerate=sample_rate, subtype=\"PCM_16\")\n",
    "    print(f\"Realtime audio saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines.audio_utils import ffmpeg_microphone_live\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# save real-time audio\n",
    "def save_realtime_audio(audio_cache, sample_rate=16000, output_path=\"realtime_audio_debug.wav\"):\n",
    "    \"\"\"\n",
    "    save real-time audio to WAV file\n",
    "    Args:\n",
    "        audio_cache (list): real-time recording cache, each element is bytes type\n",
    "        sample_rate (int): audio sample rate, default 16kHz\n",
    "        output_path (str): path of the audio file\n",
    "    \"\"\"\n",
    "    # merge all audio blocks\n",
    "    full_audio_data = b\"\".join(audio_cache)\n",
    "\n",
    "    # convert bytes stream to NumPy array\n",
    "    audio_array = np.frombuffer(full_audio_data, dtype=np.int16)\n",
    "\n",
    "    # save as WAV file\n",
    "    sf.write(output_path, audio_array, samplerate=sample_rate, subtype=\"PCM_16\")\n",
    "    print(f\"Realtime audio saved to: {output_path}\")\n",
    "\n",
    "\n",
    "# real-time recording, verification and transcription\n",
    "def transcribe_with_speaker_verification(chunk_length_s=10.0, stream_chunk_s=1.0, max_duration=30, threshold=0.4):\n",
    "    mic = ffmpeg_microphone_live(\n",
    "        sampling_rate=16000,\n",
    "        chunk_length_s=chunk_length_s,\n",
    "        stream_chunk_s=stream_chunk_s,\n",
    "    )\n",
    "\n",
    "    print(f\"Listening for speaker ...\")\n",
    "    audio_cache = []  # for caching audio blocks\n",
    "    full_audio = b\"\"  # for accumulating full audio\n",
    "    identified_user = None  # final identified user\n",
    "    start_time = time.time()  # record start time\n",
    "\n",
    "    try:\n",
    "        for i, audio_chunk in enumerate(mic):\n",
    "            elapsed_time = time.time() - start_time  # calculate elapsed time\n",
    "            if elapsed_time >= max_duration:\n",
    "                print(\"Reached max duration, stopping recording.\")\n",
    "                break\n",
    "\n",
    "            print(f\"Processing chunk {i + 1} (elapsed time: {elapsed_time:.2f}s)...\")\n",
    "            raw_audio = audio_chunk[\"raw\"]  # get audio bytes stream\n",
    "\n",
    "            # check and convert audio block to bytes stream\n",
    "            if isinstance(raw_audio, np.ndarray):\n",
    "                if raw_audio.dtype == np.float32:  # if float, convert to int16\n",
    "                    raw_audio = (raw_audio * 32768).astype(np.int16)\n",
    "                raw_audio = raw_audio.tobytes()\n",
    "            \n",
    "            # cache audio block\n",
    "            audio_cache.append(raw_audio)\n",
    "            full_audio += raw_audio  # accumulate full audio\n",
    "\n",
    "            # verify user's voice\n",
    "            audio_array = np.frombuffer(raw_audio, dtype=np.int16)\n",
    "            similarity_scores = {}\n",
    "            for speaker_name, embedding_vector in registered_speakers.items():\n",
    "                test_embeddings = speaker_model.encode_batch(torch.tensor([audio_array]))\n",
    "                test_embeddings = test_embeddings.squeeze().detach().numpy()\n",
    "                similarity = 1 - cosine(test_embeddings, embedding_vector)\n",
    "                similarity_scores[speaker_name] = similarity\n",
    "            \n",
    "            # find the most similar user\n",
    "            best_match = max(similarity_scores, key=similarity_scores.get)\n",
    "            best_score = similarity_scores[best_match]\n",
    "            print(f\"Similarity score with '{best_match}': {best_score}\")\n",
    "\n",
    "            if best_score >= threshold:\n",
    "                print(f\"Voice is from '{best_match}', welcome back, Say 'Hi ZZX'!\")\n",
    "                identified_user = best_match\n",
    "                break\n",
    "            else:\n",
    "                print(\"Voice not recognized. Continuing to listen...\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Recording interrupted by user.\")\n",
    "\n",
    "    finally:\n",
    "        # save cached audio (whether verification is successful or not)\n",
    "        if audio_cache:\n",
    "            output_path = \"/Users/7one/Documents/Work/mangoesai/livekit_paddle/realtime_audio_debug.wav\"\n",
    "            save_realtime_audio(audio_cache, sample_rate=16000, output_path=output_path)\n",
    "\n",
    "        if identified_user:\n",
    "            return identified_user\n",
    "        else:\n",
    "            print(\"No speaker identified.\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "register speakers, you can upload and build your user's voice as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rp/s3c7_7cn6sg2s70tp08w3xwm0000gn/T/ipykernel_93328/4217213878.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  audio_signal = torch.tensor([load_audio(audio_path)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker 'Dengfeng' registered successfully!\n",
      "Speaker 'Amit' registered successfully!\n"
     ]
    }
   ],
   "source": [
    "register_speaker(\"/Users/7one/Documents/Work/mangoesai/livekit_paddle/99.wav\", \"Dengfeng\")\n",
    "register_speaker(\"/Users/7one/Documents/Work/mangoesai/livekit_paddle/amit.wav\", \"Amit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listening for speaker ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 22:17:42.411 ffmpeg[93336:6186737] WARNING: Add NSCameraUseContinuityCameraDeviceType to your Info.plist to use AVCaptureDeviceTypeContinuityCamera.\n",
      "2025-01-27 22:17:44.997 ffmpeg[93336:6186737] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1 (elapsed time: 6.37s)...\n",
      "Similarity score with 'Dengfeng': 0.45401132106781006\n",
      "Voice is from 'Dengfeng', welcome back, Say 'Hi ZZX'!\n",
      "Realtime audio saved to: /Users/7one/Documents/Work/mangoesai/livekit_paddle/realtime_audio_debug.wav\n",
      "Hello, Dengfeng!\n"
     ]
    }
   ],
   "source": [
    "identified_user = transcribe_with_speaker_verification(\n",
    "    chunk_length_s=10.0,\n",
    "    stream_chunk_s=1.0,\n",
    "    max_duration=30,\n",
    "    threshold=0.4\n",
    ")\n",
    "if identified_user:\n",
    "    print(f\"Hello, {identified_user}!\")\n",
    "else:\n",
    "    print(\"No speaker recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion: \n",
    "- FAISS\n",
    "- try it on live-stream, if it's a new voice in the radio, register it , if it's "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
